{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokinizer trained, now splitting sentence and put into \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "\n",
    "# This script requires NLTK gutenberg data, uncomment this to download if not yet downloaded\n",
    "# import nltk\n",
    "# nltk.download('gutenberg')\n",
    "\n",
    "text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    text += gutenberg.raw(file_id)\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "\n",
    "# Section to preview the tokenizer\n",
    "\"\"\"\n",
    "# Test the tokenizer on a piece of text\n",
    "sentences = \"Mr. James told me Dr. Brown is not available today. I will try tomorrow.\"\n",
    "\n",
    "print (tokenizer.tokenize(sentences))\n",
    "# ['Mr. James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n",
    "\n",
    "# View the learned abbreviations\n",
    "print\n",
    "tokenizer._params.abbrev_types\n",
    "# set([...])\n",
    "\n",
    "# Here's how to debug every split decision\n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print\n",
    "    '=' * 30\n",
    "\n",
    "print\n",
    "len(text)  # 11793318\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_excel('IMDb_train_new.xlsx')\n",
    "print('tokinizer trained, now splitting sentence and put into ')\n",
    "#df['split_sentence'] = df['sentence'].apply(lambda x : tokenizer.tokenize(x))\n",
    "#print(df)\n",
    "series = []\n",
    "for sentence in df['sentence']:\n",
    "    first_split = tokenizer.tokenize(sentence)\n",
    "    for second_split in first_split:\n",
    "        series.append(second_split)\n",
    "ser = pd.Series(series)\n",
    "ser.to_csv(r'./train_split.csv', index=False, encoding='utf-8')\n",
    "#df.to_pickle(\"./IMDb_train_new.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokinizer trained, now splitting sentence and put into \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('IMDb_test_new.xlsx')\n",
    "print('tokinizer trained, now splitting sentence and put into ')\n",
    "#df['split_sentence'] = df['sentence'].apply(lambda x : tokenizer.tokenize(x))\n",
    "#print(df)\n",
    "series = []\n",
    "for sentence in df['sentence']:\n",
    "    first_split = tokenizer.tokenize(sentence)\n",
    "    for second_split in first_split:\n",
    "        series.append(second_split)\n",
    "ser = pd.Series(series)\n",
    "ser.to_csv(r'./test_split.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('IMDb_test_new.xlsx')\n",
    "df['split_sentence'] = df['sentence'].apply(lambda x : tokenizer.tokenize(x))\n",
    "#\n",
    "print(df)\n",
    "df.to_csv(r'./test_split.csv', index=False, encoding='utf-8')\n",
    "#df.to_pickle(\"./IMDb_test_new.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "df = pd.read_excel('dbpedia_20news_full.xlsx')\n",
    "\n",
    "#dask_data = dd.from_pandas(df,npartitions=20)\n",
    "#df['text'] = dask_data.apply(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', str(x)),\n",
    "#                                        axis=1, meta=('text', 'str')).compute()\n",
    "df['text'] = df['text'].apply(lambda x : re.sub(r'[^\\x00-\\x7f]',r'', str(x)))\n",
    "\n",
    "print(df['text'])\n",
    "df.to_csv(r'./imdb_train.csv', index=False, encoding='utf-8')\n",
    "#df.to_pickle(\"./IMDb_test_new.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Abbott of Farnham E D Abbott Limited was a Br...\n",
      "1          Schwan-STABILO is a German maker of pens for ...\n",
      "2          Q-workshop is a Polish company located in Poz...\n",
      "3          Marvell Software Solutions Israel known as RA...\n",
      "4          Bergan Mercy Medical Center is a hospital loc...\n",
      "                                ...                        \n",
      "648823    from email () subject what was koresh's messsa...\n",
      "648824    from email (russell turpin) subject re christi...\n",
      "648825    from email (mathew) subject re after 2000 year...\n",
      "648826    from email (patrick taylor, the sounding board...\n",
      "648827    from email (tony alicea) subject re oto, the a...\n",
      "Name: text, Length: 648828, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x : re.sub(r'[^\\x00-\\x7f]',r'', str(x)))\n",
    "\n",
    "print(df['text'])\n",
    "df.to_csv(r'./imdb_train.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
